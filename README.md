# AI Orchestrator v6.5

Un orchestrateur IA autonome avec pipeline Workflow complet, exÃ©cution d'outils directe, et interface Orchestrator UI professionnelle.

## ğŸ¯ FonctionnalitÃ©s

- **Pipeline Workflow** : SPEC â†’ PLAN â†’ EXECUTE â†’ VERIFY â†’ REPAIR â†’ COMPLETE
- **72 outils** intÃ©grÃ©s (systÃ¨me, fichiers, QA, utilitaires, rÃ©seau)
- **7 outils QA** : git_status, git_diff, run_tests, run_lint, run_format, run_build, run_typecheck
- **Erreurs rÃ©cupÃ©rables** : auto-correction via search_directory/search_files
- **Streaming WebSocket** temps rÃ©el avec run_id et phases
- **Run Inspector** : stepper workflow, tabs Tools/Thinking/QA/Raw, verdict PASS/FAIL
- **ExÃ©cution directe** : commandes sur le systÃ¨me hÃ´te (pas de sandbox)
- **Multi-modÃ¨les** : Ollama local + proxies cloud

## ğŸ—ï¸ Architecture v6.5

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVEUR (lalpha-server-1)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   Frontend      â”‚    â”‚   Backend       â”‚                â”‚
â”‚  â”‚   (Docker)      â”‚    â”‚   (systemd)     â”‚                â”‚
â”‚  â”‚   nginx:alpine  â”‚â”€â”€â”€â–¶â”‚   Python/FastAPIâ”‚                â”‚
â”‚  â”‚   Port 8002     â”‚    â”‚   Port 8001     â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                  â”‚                          â”‚
â”‚                                  â–¼                          â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                         â”‚    Ollama       â”‚                â”‚
â”‚                         â”‚   (systemd)     â”‚                â”‚
â”‚                         â”‚   Port 11434    â”‚                â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pourquoi pas Docker pour le backend ?

- **Connexion Ollama simplifiÃ©e** : localhost:11434 direct
- **AccÃ¨s systÃ¨me complet** : git, python3, npm sans restrictions
- **Pas de problÃ¨mes rÃ©seau** : plus de host.docker.internal
- **Performance** : pas d'overhead conteneur

## ğŸš€ Installation

### PrÃ©requis

- Ubuntu 22.04+ / Debian 12+
- Python 3.11+
- Node.js 18+
- Ollama installÃ© et fonctionnel
- Docker (uniquement pour le frontend)

### Backend (systemd)

```bash
cd /home/lalpha/projets/ai-tools/ai-orchestrator/backend

# Installer les dÃ©pendances
pip install -r requirements.txt --break-system-packages

# Configurer
cp .env.example .env
nano .env  # Ã‰diter les valeurs

# CrÃ©er le service systemd
sudo tee /etc/systemd/system/ai-orchestrator.service << 'SYSTEMD'
[Unit]
Description=AI Orchestrator Backend v6.5
After=network.target ollama.service

[Service]
Type=simple
User=lalpha
WorkingDirectory=/home/lalpha/projets/ai-tools/ai-orchestrator/backend
Environment=PATH=/home/lalpha/.local/bin:/usr/bin:/bin
ExecStart=/usr/bin/python3 -m uvicorn main:app --host 0.0.0.0 --port 8001
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
SYSTEMD

# Activer et dÃ©marrer
sudo systemctl daemon-reload
sudo systemctl enable ai-orchestrator
sudo systemctl start ai-orchestrator
```

### Frontend (Docker)

```bash
cd /home/lalpha/projets/ai-tools/ai-orchestrator/frontend

# Build
npm install
npm run build

# Le conteneur nginx est gÃ©rÃ© par unified-stack
# Voir /home/lalpha/projets/infrastructure/unified-stack/
```

## ğŸ”§ Configuration

### Variables d'environnement (backend/.env)

```env
# AI Orchestrator Backend v6.5
APP_VERSION=6.5
APP_NAME=AI Orchestrator

# Ollama - connexion directe
OLLAMA_URL=http://localhost:11434

# ModÃ¨les
DEFAULT_MODEL=kimi-k2:1t-cloud
EXECUTOR_MODEL=kimi-k2:1t-cloud

# SÃ©curitÃ©
JWT_SECRET_KEY=your-secret-key
ADMIN_PASSWORD=your-password

# Base de donnÃ©es
DATABASE_URL=sqlite:///./ai_orchestrator.db

# Workflow
VERIFY_REQUIRED=false
MAX_REPAIR_CYCLES=2
MAX_ITERATIONS=10

# ExÃ©cution directe (pas de sandbox Docker)
EXECUTE_MODE=direct
```

### Configuration nginx (frontend)

Le frontend utilise nginx pour servir les fichiers statiques et proxy vers le backend :

```nginx
location /api/ {
    proxy_pass http://10.10.10.46:8001/api/;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";
}
```

## ğŸ“‹ Commandes de gestion

### Backend (systemd)

```bash
# Status
sudo systemctl status ai-orchestrator

# Logs en temps rÃ©el
journalctl -u ai-orchestrator -f

# RedÃ©marrer
sudo systemctl restart ai-orchestrator

# ArrÃªter
sudo systemctl stop ai-orchestrator
```

### Frontend (Docker via unified-stack)

```bash
cd /home/lalpha/projets/infrastructure/unified-stack

# RedÃ©marrer le frontend
docker restart ai-orchestrator-frontend

# Logs
docker logs -f ai-orchestrator-frontend
```

## ğŸ“¡ API Endpoints

| Endpoint | Description |
|----------|-------------|
| `POST /api/v1/auth/login` | Authentification |
| `GET /api/v1/conversations` | Liste conversations |
| `WS /api/v1/chat/ws` | Chat streaming |
| `GET /api/v1/system/health` | Health check |
| `GET /api/v1/system/models` | ModÃ¨les disponibles |
| `GET /api/v1/tools` | Liste des outils |

## ğŸ”’ SÃ©curitÃ©

### Mode d'exÃ©cution

| Mode | Description | Usage |
|------|-------------|-------|
| `direct` | ExÃ©cution sur le systÃ¨me hÃ´te | **RecommandÃ©** pour serveur personnel |
| `sandbox` | Conteneur Docker isolÃ© | Pour environnements multi-utilisateurs |

### Allowlist de commandes

Les commandes sont filtrÃ©es par une allowlist dans `config.py`. Commandes autorisÃ©es :
- SystÃ¨me : `uname`, `hostname`, `uptime`, `free`, `df`, `ps`
- Fichiers : `ls`, `cat`, `head`, `tail`, `grep`, `find`
- Dev : `python3`, `pip3`, `node`, `npm`, `git`
- QA : `ruff`, `black`, `mypy`, `pytest`

## ğŸ“š Documentation

- [API Reference](docs/API.md)
- [Architecture](docs/ARCHITECTURE.md)
- [Tools](docs/TOOLS.md)
- [WebSocket Protocol](docs/WEBSOCKET.md)
- [Troubleshooting](docs/TROUBLESHOOTING.md)

## ğŸ“ Changelog

### v6.5 (2026-01-09)
- **Architecture** : Backend en systemd (plus de Docker)
- **ExÃ©cution** : Mode direct par dÃ©faut
- **Ollama** : Connexion localhost directe
- **Fix** : Dropdown modÃ¨les affiche les noms correctement
- **Fix** : normalize_model() pour SQLite

Voir [CHANGELOG.md](docs/CHANGELOG.md) pour l'historique complet.

## ğŸ“„ Licence

MIT License - Voir [LICENSE](LICENSE)
